{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression. Minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must always import the relevant libraries for our problem at hand. NumPy is a must for this example.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib and mpl_toolkits are not necessary. We employ them for the sole purpose of visualizing the results.  \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random input data to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# First, we should declare a variable containing the size of the training set we want to generate.\n",
    "observations = 1000\n",
    "\n",
    "# We will work with two variables as inputs. You can think about them as x1 and x2 in our previous examples.\n",
    "# We have picked x and z, since it is easier to differentiate them.\n",
    "# We generate them randomly, drawing from an uniform distribution. There are 3 arguments of this method (low, high, size).\n",
    "# The size of xs and zs is observations by 1. In this case: 1000 x 1.\n",
    "xs = np.random.uniform(low=-10, high=10, size=(observations,1))\n",
    "zs = np.random.uniform(-10, 10, (observations,1))\n",
    "\n",
    "# Combine the two dimensions of the input into one input matrix. \n",
    "# This is the X matrix from the linear model y = x*w + b.\n",
    "# column_stack is a Numpy method, which combines two vectors into a matrix. Alternatives are stack, dstack, hstack, etc.\n",
    "inputs = np.column_stack((xs,zs))\n",
    "\n",
    "# Check if the dimensions of the inputs are the same as the ones we defined in the linear model lectures. \n",
    "# They should be n x k, where n is the number of observations, and k is the number of variables, so 1000 x 2.\n",
    "print (inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the targets we will aim at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# We want to \"make up\" a function, use the ML methodology, and see if the algorithm has learned it.\n",
    "# We add a small random noise to the function i.e. f(x,z) = 2x - 3z + 5 + <small noise>\n",
    "noise = np.random.uniform(-1, 1, (observations,1))\n",
    "\n",
    "# Produce the targets according to the f(x,z) = 2x - 3z + 5 + noise definition.\n",
    "# In this way, we are basically saying: the weights should be 2 and -3, while the bias is 5.\n",
    "targets = 2*xs - 3*zs + 5 + noise\n",
    "\n",
    "# Check the shape of the targets just in case. It should be n x m, where m is the number of output variables, so 1000 x 1.\n",
    "print (targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training data\n",
    "The point is to see that there is a strong trend that our model should learn to reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use the 3D plot, the objects should have a certain shape, so we reshape the targets.\n",
    "# The proper method to use is reshape and takes as arguments the dimensions in which we want to fit the object.\n",
    "targets = targets.reshape(observations,)\n",
    "\n",
    "# Plotting according to the conventional matplotlib.pyplot syntax\n",
    "\n",
    "# Declare the figure\n",
    "# fig = plt.figure()\n",
    "\n",
    "# A method allowing us to create the 3D plot\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Choose the axes.\n",
    "# ax.plot(xs, zs, targets)\n",
    "\n",
    "# Set labels\n",
    "# ax.set_xlabel('xs')\n",
    "# ax.set_ylabel('zs')\n",
    "# ax.set_zlabel('Targets')\n",
    "\n",
    "# You can fiddle with the azim parameter to plot the data from different angles. Just change the value of azim=100\n",
    "# to azim = 0 ; azim = 200, or whatever. Check and see what happens.\n",
    "# ax.view_init(azim=100)\n",
    "\n",
    "# So far we were just describing the plot. This method actually shows the plot. \n",
    "# plt.show()\n",
    "\n",
    "# We reshape the targets back to the shape that they were in before plotting.\n",
    "# This reshaping is a side-effect of the 3D plot. Sorry for that.\n",
    "targets = targets.reshape(observations,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04214589]\n",
      " [0.09683935]]\n",
      "[0.07617405]\n"
     ]
    }
   ],
   "source": [
    "# We will initialize the weights and biases randomly in some small initial range.\n",
    "# init_range is the variable that will measure that.\n",
    "# You can play around with the initial range, but we don't really encourage you to do so.\n",
    "# High initial ranges may prevent the machine learning algorithm from learning.\n",
    "init_range = 0.1\n",
    "\n",
    "# Weights are of size k x m, where k is the number of input variables and m is the number of output variables\n",
    "# In our case, the weights matrix is 2x1 since there are 2 inputs (x and z) and one output (y)\n",
    "weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))\n",
    "\n",
    "# Biases are of size 1 since there is only 1 output. The bias is a scalar.\n",
    "biases = np.random.uniform(low=-init_range, high=init_range, size=1)\n",
    "\n",
    "#Print the weights to get a sense of how they were initialized.\n",
    "print (weights)\n",
    "print (biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some small learning rate (denoted eta in the lecture). \n",
    "# 0.02 is going to work quite well for our example. Once again, you can play around with it.\n",
    "# It is HIGHLY recommended that you play around with it.\n",
    "learning_rate = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239.44811039892863\n",
      "35.10481148907469\n",
      "13.797136454318109\n",
      "11.219637849615268\n",
      "10.573341962566095\n",
      "10.139744168634813\n",
      "9.742459905399027\n",
      "9.362864647863761\n",
      "8.99849866868076\n",
      "8.648578820989927\n",
      "8.31251470023228\n",
      "7.989755852032856\n",
      "7.679775315108673\n",
      "7.382067165571582\n",
      "7.096145527423658\n",
      "6.821543762746321\n",
      "6.5578137083916825\n",
      "6.304524944380178\n",
      "6.061264091416244\n",
      "5.827634136231736\n",
      "5.603253783641223\n",
      "5.387756834250113\n",
      "5.180791586799849\n",
      "4.982020264174723\n",
      "4.791118462133607\n",
      "4.607774619866799\n",
      "4.4316895115139925\n",
      "4.262575757813452\n",
      "4.100157357085409\n",
      "3.9441692347842046\n",
      "3.7943568108840493\n",
      "3.650475584392318\n",
      "3.512290734312306\n",
      "3.3795767364042364\n",
      "3.252116995118955\n",
      "3.1297034901037217\n",
      "3.012136436703117\n",
      "2.89922395990098\n",
      "2.7907817811712747\n",
      "2.6866329177267785\n",
      "2.586607393674775\n",
      "2.490541962608306\n",
      "2.398279841180297\n",
      "2.309670453225672\n",
      "2.224569184013913\n",
      "2.1428371442309406\n",
      "2.064340943305155\n",
      "1.9889524717076945\n",
      "1.9165486918715902\n",
      "1.8470114373886153\n",
      "1.7802272201560962\n",
      "1.7160870451589323\n",
      "1.6544862325845557\n",
      "1.5953242469805062\n",
      "1.5385045331757952\n",
      "1.4839343586982776\n",
      "1.4315246624308573\n",
      "1.3811899092595081\n",
      "1.3328479504759099\n",
      "1.2864198897068464\n",
      "1.2418299541515843\n",
      "1.1990053709170554\n",
      "1.1578762482490457\n",
      "1.118375461465528\n",
      "1.080438543405988\n",
      "1.0440035792179658\n",
      "1.009011105309066\n",
      "0.9754040122995582\n",
      "0.9431274518171476\n",
      "0.9121287469818321\n",
      "0.8823573064347334\n",
      "0.8537645417705878\n",
      "0.8263037882391686\n",
      "0.7999302285861898\n",
      "0.7746008199094275\n",
      "0.750274223410649\n",
      "0.7269107369287364\n",
      "0.7044722301438731\n",
      "0.6829220823470457\n",
      "0.6622251226733115\n",
      "0.6423475727012794\n",
      "0.6232569913251118\n",
      "0.6049222218091074\n",
      "0.5873133409384252\n",
      "0.5704016101829759\n",
      "0.5541594287947784\n",
      "0.538560288762222\n",
      "0.5235787315477399\n",
      "0.5091903065382603\n",
      "0.4953715311406458\n",
      "0.4820998524569901\n",
      "0.46935361047721125\n",
      "0.4571120027288896\n",
      "0.4453550503266427\n",
      "0.43406356536563895\n",
      "0.4232191196060199\n",
      "0.41280401439714265\n",
      "0.40280125179253723\n",
      "0.3931945068084538\n",
      "0.3839681007807103\n"
     ]
    }
   ],
   "source": [
    "# We iterate over our training dataset 100 times. That works well with a learning rate of 0.02.\n",
    "# The proper number of iterations is something we will talk about later on, but generally\n",
    "# a lower learning rate would need more iterations, while a higher learning rate would need less iterations\n",
    "# keep in mind that a high learning rate may cause the loss to diverge to infinity, instead of converge to 0.\n",
    "for i in range (100):\n",
    "    \n",
    "    # This is the linear model: y = xw + b equation\n",
    "    outputs = np.dot(inputs,weights) + biases\n",
    "    # The deltas are the differences between the outputs and the targets\n",
    "    # Note that deltas here is a vector 1000 x 1\n",
    "    deltas = outputs - targets\n",
    "        \n",
    "    # We are considering the L2-norm loss, but divided by 2, so it is consistent with the lectures.\n",
    "    # Moreover, we further divide it by the number of observations.\n",
    "    # This is simple rescaling by a constant. We explained that this doesn't change the optimization logic,\n",
    "    # as any function holding the basic property of being lower for better results, and higher for worse results\n",
    "    # can be a loss function.\n",
    "    loss = np.sum(deltas ** 2) / 2 / observations\n",
    "    \n",
    "    # We print the loss function value at each step so we can observe whether it is decreasing as desired.\n",
    "    print (loss)\n",
    "    \n",
    "    # Another small trick is to scale the deltas the same way as the loss function\n",
    "    # In this way our learning rate is independent of the number of samples (observations).\n",
    "    # Again, this doesn't change anything in principle, it simply makes it easier to pick a single learning rate\n",
    "    # that can remain the same if we change the number of training samples (observations).\n",
    "    # You can try solving the problem without rescaling to see how that works for you.\n",
    "    deltas_scaled = deltas / observations\n",
    "    \n",
    "    # Finally, we must apply the gradient descent update rules from the relevant lecture.\n",
    "    # The weights are 2x1, learning rate is 1x1 (scalar), inputs are 1000x2, and deltas_scaled are 1000x1\n",
    "    # We must transpose the inputs so that we get an allowed operation.\n",
    "    weights = weights - learning_rate * np.dot(inputs.T,deltas_scaled)\n",
    "    biases = biases - learning_rate * np.sum(deltas_scaled)\n",
    "    \n",
    "    # The weights are updated in a linear algebraic way (a matrix minus another matrix)\n",
    "    # The biases, however, are just a single number here, so we must transform the deltas into a scalar.\n",
    "    # The two lines are both consistent with the gradient descent methodology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print weights and biases and see if we have worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.99900289]\n",
      " [-2.99826107]] [4.35613389]\n"
     ]
    }
   ],
   "source": [
    "# We print the weights and the biases, so we can see if they have converged to what we wanted.\n",
    "# When declared the targets, following the f(x,z), we knew the weights should be 2 and -3, while the bias: 5.\n",
    "print (weights, biases)\n",
    "\n",
    "# Note that they may be convergING. So more iterations are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot last outputs vs targets\n",
    "Since they are the last ones at the end of the training, they represent the final model accuracy. <br/>\n",
    "The closer this plot is to a 45 degree line, the closer target and output values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We print the outputs and the targets in order to see if they have a linear relationship.\n",
    "# Again, that's not needed. Moreover, in later lectures, that would not even be possible.\n",
    "plt.plot(outputs,targets)\n",
    "plt.xlabel('outputs')\n",
    "plt.ylabel('targets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
